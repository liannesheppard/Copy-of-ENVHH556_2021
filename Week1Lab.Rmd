---
title: "ENVH 556 Week 1 Lab:  Becoming familiar with R, RStudio, RMarkdown"
author: "ENVH 556 Instructors"
date: "Created for Winter 2021; updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        toc: true
        toc_depth: 3
        number_sections: true
---

---

Note:  The code in the following three chunks should typically appear at the beginning of
each R Markdown file.  We display them during this first lab so you can see them
in the output.  Ordinarily we would not show them.

```{r setup, include=TRUE}
#-----setup-------------

# This chunk won't appear in your output when knitting because include=FALSE
# (which hides everything).
#
# We are showing the code in this chunk in the first lab; ordinarily
# we will choose to hide it with the option echo=FALSE. 
#
# Note about naming chunks:
#
# The chunk name is useful for reading code, for the index you can choose to
# show in RStudio to the right of the script editor, and for more advanced
# purposes.  The chunk name will not show up in your Appendix code compilation
# however.  The comment with the chunk name between dashes will both show up in
# the index (boldfaced) and print in your appendix.  This facilitates review of
# the code. I recommend using chunk names and putting them at least as a
# comment at the beginning of the chunk.
#
# For more about `knitr` chunk options see: https://yihui.org/knitr/options/

knitr::opts_chunk$set(
    echo = TRUE,
    cache = FALSE,
    cache.comments = FALSE,
    message = FALSE,
    warning = FALSE
)

```

```{r clear.workspace, eval=FALSE, echo=TRUE}
#-----clear workspace------------

# Clear the environment without clearing knitr
#
# This chunk is useful for code development because it simulates the knitr
# environment. Run it as a code chunk when testing. When knitr is run, it uses a
# fresh, clean environment, so we set eval=FALSE to disable this chunk when
# rendering.

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.packages.with.pacman, include=TRUE}
#-----load packages with pacman--------------

# Ordinarily we set the chunk header to include=FALSE for this chunk; just 
# showing it for the first lab.

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  As you progress, it is good
# practice to load specific packages, but for now we'll load all of the 
# `tidyverse` for convenience. Some reasons for packages:
# knitr:  kable()
# ggplot2: Don't load:  part of tidyverse
# readr:   Don't load:  part of tidyverse
# dplyr:   Don't load:  part of tidyverse
# Hmisc:  Miscellaneous functions helpful for data analysis
# EnvStats: geoMean, geoSD, probability plotting functions
pacman::p_load(tidyverse, knitr, Hmisc, EnvStats)

```

---

# Goal

The goal of this lab is to help everyone become familiar with R, R Studio, and R
Markdown through the context of the course content.  This is also your first
opportunity to develop a lab write-up for ENVH 556 using R Markdown.

Note:  It is in your best interest to learn modern R tools, such as those
included in the [`tidyverse`](https://www.tidyverse.org).  In this class we tend 
to prioritize `tidyvrese` approaches, but recognize there are often many packages 
and options to accomplish the same task. We want to deliver course materials 
without getting too bogged down in the debate about the "best" or "highest 
performing" packages.

---

# Practice Session

This section covers basic practice to be completed during the lab.  It will
introduce you to multiple useful commands and help you get a basic understanding
of the dataset.

The underlying data to be used in the lab is from the DEMS study and is
described in [Coble et al 2010](https://academic.oup.com/annweh/article/54/7/747/202635).  We are focusing on the personal data collected
by NIOSH between 1998 and 2001.  In this lab we will mostly use the `ecdata`
variable which is a measurement of exposure to respirable elemental carbon, also
called REC.  The data were collected from workers from a cross-section of jobs
at the seven mines open during the data collection.  For further information on
this dataset, see the document: *DEMS Personal Data overview* available on the
class Canvas site.

A big part of data science is data management and ideally you will also learn
basic data management in conjunction with your work.  This class will not
emphasize data management, although you will do some during your term project.
Note that some of the habits we stress, including those we discuss in this lab,
are very useful for both data management and data analyses.

## Set up a RStudio project, file paths, and read the data

Complete the following steps to set up the lab  

* Start a new R project in R Studio.  
    + Note:  Put your project for this course (and lab) in a sensible directory.  
    + Create a subdirectory of that directory with a name such as `Datasets` and
      copy the R dataset *DEMSCombinedPersonal.rds* into it. (This lab assumes
      your data directory is named `Datasets`.)
    + Copy this lab's `.Rmd` file, *Week1Lab.Rmd* into your project directory
      and open it into RStudio.
    + Note:  Often you will open a new `.Rmd` (R Markdown) file and populate it
      with code and text for this course, such as from the
      *LabReportGuidelines.Rmd* file.

* In the R Markdown file you are using for this lab make sure you have your file
paths set up (see *set file paths* chunk below).
    + First assign your current path to a variable name.  
    + Then tell R where your data reside.  The `dir.create()` command will make 
    a directory if it doesn't exist.  The `file.path()` command allows you to 
    refer easily to paths across operating systems.
    
```{r set.file.paths}
#-----set file paths--------

# create a variable for the project path
project_path <- getwd()

# create a variable for the data path
data_path <- file.path(project_path, "Datasets")

# create directory, if one does not already exist
dir.create(data_path, showWarnings = FALSE, recursive = TRUE)

```

* Read in the data 
     + The data we will use have already been converted to an R dataset.  Make 
     sure the "DEMSCombinedPersonal.rds" dataset is in your Datasets directory 
     defined by `data_path`.
     
```{r read.data, eval=FALSE, error=TRUE}
#-----read data--------

# Note 1:  for simplicity of typing in this lab we're calling the dataset DEMS,
# even though it is only one of several DEMS datasets we will use in this
# course. 
# Note 2: In this chunk heading eval = FALSE so that if it is run, we don't get an 
# error if the file is not present in the "Datasets" folder. This would work if 
# the `.rds` file were already downloaded and is a bit simpler compared to the 
# more robust but more complicated code below.  
DEMS <- readRDS(file.path(data_path, "DEMSCombinedPersonal.rds"))

```

As an alternative, the following code checks to see if the file is already 
present, and if not, downloads the data from a web site before reading it in:

```{r read.data.from.a.website, eval=TRUE}
#-----read data from a website--------

# first, specify file name and path
DEMS_file <- "DEMSCombinedPersonal.rds"
DEMS_path <- file.path(data_path, DEMS_file)

# Only download the file if it's not already present
if (!file.exists(DEMS_path)) {
    url <- paste("https://staff.washington.edu/high/envh556/Datasets", 
                 DEMS_file, sep = '/')
    download.file(url = url, destfile = DEMS_path)
}

# Output a warning message if the file cannot be found
if (file.exists(DEMS_path)) {
    DEMS <- readRDS(file = DEMS_path)
} else warning(paste("Can't find", DEMS_file, "!"))

# remove temporary variables
rm(url, DEMS_file, DEMS_path)

```

## Check your data first!

An essential part of any data analysis is making sure the data you are analyzing
are as you expect.  Before you start any new data analysis, make sure your data
have been read in correctly and get a basic understanding of their structure.

### Check to make sure your data have been read in correctly 

Here are some basic questions you should ask every time you read in a new
dataset:

  * How many observations are in your dataset?  Does this number correspond to
  the originating dataset?
  * What are the variable names in the file? How are they formatted?  

#### Here are some commands that will allow you to answer these questions:

From base R:

  * `class(DEMS)` tells you what class the object DEMS belongs to
  * `names(DEMS)` lists the variable names  
  * `dim(DEMS)` gives the dimensions of DEMS
  * `sapply(DEMS, class)` tells you the class of every variable (column)
  * `typeof(DEMS)` says what is the storage mode or R internal type of this object
  * `View(DEMS)`  to open a browser to look at the entire dataset  

From `tidyverse`:

  * `(DEMS <- as_tibble(DEMS))`  create a tibble version and look at DEMS in the 
    tibble format
  * `select(DEMS, facilityno, ecdata, nodata, no2data)` allows you to zoom in on 
    specific variables in the DEMS tibble.

Replace the comment in the chunk below with some of these commands.

```{r data.description.1}
#-----data description 1---------- 

# STUDENTS may want to edit this chunk to look at the data differently

# Here is a first pass at commands to use to check your data we expect this to 
# be a data frame
class(DEMS) 

# We expect this to have 1275 rows and 17 columns
dim(DEMS) 

# This list should correspond to the list in the data summary document:
names(DEMS) 

# This gives us details on the type of each variable in the dataset
sapply(DEMS, class) 

# Now for a tidyverse option.  We will also save this version in case we want to
# use tidverse commands.

# Now lets make the DEMS data frame into a tibble Tibbles are tidyverse 
# dataframes with some extra functionality and a friendly display. Occasionally 
# you will run into functions that will work with "data.frame" objects but not 
# "tibble" objects. 
# (The parentheses surrounding the command tell R to print the result of the 
#assignment.)
(DEMS <- as_tibble(DEMS)) 

# Our application of select zooms in to focus on the key exposure data
select(DEMS, facilityno, u_s, ecdata, nodata, no2data) 

```

### Get some basic understanding of the data 

Here are some commands:

  * `head(DEMS)` shows the first 6 rows of the dataset
  * `tail(DEMS, 1)` shows the last row of the dataset
  * `head(DEMS$facilityno, 20)` shows the first 20 rows of a subset of variables
  * `summary(DEMS)` gives a basic summary of each variable
  * `describe(DEMS)` is another useful basic summary of the dataset, from the 
    Hmisc package ("a concise statistical description")
  * `xtabs(~facilityno, data=DEMS)` Note: `faciltyno` is a factor, ordered by 
    facility number
  * `xtabs(~facilityid, data=DEMS)` Note: `facilityid` is a character variable, 
    ordered by facility letter


```{r data.description.2}
#-----data description 2-------

# Here is a first pass at commands for students to try to better understand the data
# # first 6 rows and ALL variables (columns):
head(DEMS) 

# a basic summary of each variable
summary(DEMS) 

# a different basic summary of each variable from `Hmisc`
describe(DEMS) 

# tallies of the number of observations by facilityno
xtabs( ~ facilityno, data = DEMS) 

```


### Verify your data correspond to what you expect 

The specifics of what you evaluate depend upon the context of the problem.  In
our case we have published papers we can rely upon to determine whether our data
are what we expect.  Here are some questions for the DEMS personal data:

  * How many observations are in your dataset overall and by facility?  Do these
numbers correspond to the originating datasets?  (You can check against the
papers.)
  * Note that there are missing data in this dataset.  As you proceed, you will
want to verify whether they are consistent with your expectations.  Presence of
missing data requires additional attention to how you will handle these values
in R.
    + Which variables have missing values?
    + Which variable(s) have the most missing data?

```{r data.description.3}
#-----data description 3------------------

# STUDENTS ADD code to look at the data:
# Which commands from above will allow you to verify observation numbers and
# missing value counts?
# In this dataset it is important to understand summaries within mines.  Which
# commands from above will facilitate that?  (We will learn more below too.)

```

## Basic data description

Once you have a basic understanding of your data and believe they were read in
correctly, you can focus on quantities of interest.  We will focus on creating a
few basic summaries of `ecdata` in this lab.  Next week you will get more
experience with other tools such as transformations and checking the
distirbution of variables.

### Tables and computing descriptive statistics 

  * What do you observe about the distribution?  
  * Do the descriptive statistics vary by facility?  

Here are some commands:

  * `min(DEMS$ecdata,na.rm=TRUE)` is the minimum after missing values omitted
  * `mean(DEMS$ecdata,na.rm=TRUE)` is the mean after missing values omitted
  * `sd(DEMS$ecdata,na.rm=TRUE)` is the standard deviation after missing values 
    omitted
  * `summaryFull(DEMS[,c("ecdata","nodata","no2data")])` is a full set of summary
    statistics from the EnvStats package.  It only works for numeric variables.
  * `fivenum(DEMS$ecdata)` shows the five number summary of ecdata (min, lower
    hinge (close to the 25th percentile), median, upper hinge, maximum).  Note no
    need to tell it to remove missing missing values here.  Five number summaries
    are used in box plots and give you a basic understanding of the distribution 
    of a variable.
  * From the EnvStats package, `geoMean(ecdata,na.rm=TRUE)` gives the geometric 
    mean while `geoSD(ecdata,na.rm=TRUE)` gives the GSD.

To produce summary statistics by facility:  Here are two ideas.  Which do you
prefer and why?

  * A simple table of summary statistics using `tapply` and `cbind`.  Note the
  extra attention needed to handle missing data for the sample size. (What do 
  you notice about the sample sizes here vs. when you just counted 
  observations?  What is the cause of the difference?)

```{r simple.table, echo=FALSE}
#-----simple.table using tapply and cbind ----------

#This uses facilityno, a factor variable
xbar<- tapply(DEMS$ecdata, DEMS$facilityno, mean, na.rm=TRUE)
sd  <- tapply(DEMS$ecdata, DEMS$facilityno, sd, na.rm=TRUE)
n   <- tapply(DEMS$ecdata[!is.na(DEMS$ecdata)], 
             DEMS$facilityno[!is.na(DEMS$ecdata)], length)
cbind(mean=xbar, SD=sd, N=n)

# Note you can present this table in a slightly nicer format using kable from
# the knitr package:
kable(cbind(mean=xbar, SD=sd, N=n))

```

* Here is a `tidyverse` option using `dplyr`:

```{r table.with.dplyr}
#-----table with dplyr------

# dplyr uses verbs for function names. You'll get more comfortable with them 
# as you practice. Also, watch spelling, for instance, `summarise()` versus 
# `summarize()`. In this case summarize()` may call the `Hmisc` function 
# rather than the `dplyr` function.
facility_ec <- DEMS %>% 
  group_by(facilityno) %>% 
  summarise(N = sum(!is.na(ecdata)),
            Nmiss = sum(is.na(ecdata)),
            mean = mean(ecdata, na.rm = TRUE),
            sd = sd(ecdata, na.rm = TRUE),
            se = sd/sqrt(N)
    )

# show tibble
facility_ec

# And here is the same result printed using kable
kable(facility_ec)
```

Note:  One reason to do computations in multiple ways is to help you make sure
you are reporting the correct values and that you completely understand your
output.  When multiple approaches to the same summary give the same answer, you
can be more confident that your answer is correct.

### Plots:  Histograms 

#### Make a histogram of ecdata. Add a density curve to the histogram.    

`ggplot2` (part of `tidyverse`)  has great tools for plotting.  First we
show a basic histogram.  Then we switch to the density scale and overlay a
normal density with the same mean and variance as the data.  Finally we 
overlay a density plot.

See chunk comments and the notes following the chunk for option
suggestions.

`ggplot` gives warnings about omitted data and messages about better
options to choose.  You can prevent these messages from showing up in 
your rendered document by using the chunk option `warning=FALSE` and
`message=FALSE`.  They will show up in the R Markdown console instead if 
you set these to `FALSE`.  (Note:  We set these options globally in our 
setup chunk above so we don't need to repeat them below.)

```{r hist.in.tidyverse, warning=FALSE, message=FALSE}
#-----hist in tidyverse---------

# Plot 1 with histogram only and count on the y axis (the default) the default
# binwidth is 30 and often you will want to change it; see plot 2 for a narrower
# binwidth.
ggplot(data=DEMS, aes(ecdata, na.rm=TRUE)) +
    geom_histogram(colour="black", fill="white")

# Plot 2 with density base plot, just the histogram, now assigned to object 
# named 'p' for ease of re-use and a different theme:
p <- ggplot(data=DEMS, aes(ecdata, na.rm=TRUE)) + 
    geom_histogram(aes(y=..density..), colour="black", fill="white", 
                   binwidth=20) + 
    theme_classic()

# To overlay a normal density plot, we need to create some variables first
N <- sum(!is.na(DEMS$ecdata))

# Divide the range 0-1000 into N equal increments
x <- seq(0, 1000, length.out=N) 
df <- with(DEMS[!is.na(DEMS$ecdata), ], 
           data.frame(x, y = dnorm(x, mean(ecdata), sd(ecdata))))

# add normal plot to histogram
p + geom_line(data = df, aes(x = x, y = y), color = "red")

# Plot 3 now also overlays a kernel density plot.  The alpha parameter (range
# 0-1) controls the degree of transparency, while the `fill="red"` ensures the
# overlaid density is a specific color.  We are using the default bandwidth (bw)
# here.  We've also included a few style options, such as axis expansion so the 
# bars aren't "floating" and an overall theme.
p +  geom_line(data = df, aes(x = x, y = y), color = "red") +
  geom_density(alpha=.2, fill="red") + 
  scale_y_continuous(expand = expand_scale(mult = c(0, .1))) +
  theme_bw()

```

Note:  In contrast to the normal density which overlays a probability
distribution of a specific form, kernel density smoothers give you smooth curve
that track the data.  The smooth curve will be rougher for smaller bandwidths.

To set the amount of smoothing (i.e. the bandwidth) in the kernel density, use
the `bw` option.  Kernels are scaled so that `bw` is the standard deviation of
the smoothing kernel. To choose the smoothing kernel, use the `kernel` option,
e.g. `kernel="gaussian"` (which is the default).

## Create and work with transformed data

Typically exposure data appear to be log-normally distributed.  In this section 
we transform and plot the transformed data.

### Logarithmic transformations using `dplyr`:

  * `mutate( DEMS, ln_ecdata = log(ecdata) )` creates the natural 
    log-transformed variable in the DEMS data frame.
  * `mutate( DEMS, log10_ecdata = log10(ecdata) )` creates the base 10
    log-transformed variable in the DEMS data frame.
    
Note that `mutate` adds the new variables to the end of the dataset.  For more
functions you can use with `mutate` to create new variables, see R4DS pp. 56-58.

```{r transform.vars}
#-----transform vars------

# The following two variables will be added to the DEMS dataframe at the end of
# the data frame:
DEMS <- DEMS %>% 
  mutate(ln_ecdata = log(ecdata), 
         log10_ecdata = log10(ecdata) )

```

## More plots

### Overview of `ggplot` basics

Here is the basic format of `ggplot`: 

```
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
```

where `<DATA>` is your dataset, `aes` is the aesthetic for the mapping or the
visual properties of the objects in the plot, such as size, shape, and color of
points.  The `mapping` is how you apply the variables in the dataset to the
aesthetics.  Finally, the `<GEOM_FUNCTION>` is how the mapping will be
presented.  Examples are `geom_point` and `geom_line`.  `ggplot` is very
expandable, allowing you to layer aspects into your plots, such as to overlay
density plots on top of histograms as we did above.  More generally, the layered
grammar of `ggplot` is:

```
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(
        mapping = aes(<MAPPINGS>),
        stat = <STAT>,
        position = <POSITION>
    ) +
    <COORDINATE_FUNCTION> +
    <FACET_FUNCTION> +
    <LABEL_FUNCTION>

```

where the added pieces are `<STAT>`: statistics to put on the plot, `<POSITION>`
to locate your object (such as `"jitter"`"), `<COORDINATE_FUNCTION>` to specify
the coordinate system to plot under (e.g. polar coordinates), `<FACET_FUNCTION>`
to allow you to divide the plot into subplots, and `<LABEL_FUNCTION>` in order
to add titles, axis labels, etc.

For more details on `ggplot` see R4DS, the [R for Data
Science](https://r4ds.had.co.nz/data-visualisation.html) book by Hadley Wickham 
and Garrett Grolemund.

### Scatterplots:  Now plot the relationship between NO (nodata)  and REC (ecdata). 

`ggplot2` makes this easy:  

```{r scatterplot.in.tidyverse, echo=TRUE, warning=FALSE, message=FALSE}
#-----scatterplot in tidyverse------------

# Plot 1: simple way to use ggplot to ask for a scatterplot
qplot(nodata, ecdata, data=DEMS) 

# Plot 2 is the same plot, now using standard ggplot notation
ggplot(data = DEMS, aes(nodata, ecdata, na.rm = TRUE)) + geom_point()

# Plot 3 adds a best fit line added without its 95% CI (se=FALSE option). We
# also add a title and axis labels
ggplot(data = DEMS, aes(nodata, ecdata, na.rm = TRUE)) +
    geom_point() + stat_smooth(method = lm, se = FALSE) +
    labs(title = "Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data",  
         x = "NO (ppm)",
         y = "REC (ug/m3)")

# Plot 4 replaces the best fit line with a smooth loess curve added and its 95%
# CI (the default smoother is gam for large datasets and loess when there are
# less than 1,000 observations)
ggplot(data = DEMS, aes(nodata, ecdata, na.rm = TRUE)) +
    geom_point() + stat_smooth(method = "loess") +
    labs(title = "Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data",
        x = "NO (ppm)",
        y = "REC (ug/m3)" ) 

# Plot 5 adds different colors and smoothers for underground vs. surface data
ggplot(data = DEMS, aes(nodata, ecdata, na.rm = TRUE)) +
    geom_point(mapping = aes(color = u_snum)) +
    geom_smooth(method = "loess", aes(color = u_snum)) +
    labs(
    title = paste(
        "Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data\n",
        "colored by where measurements were taken"
    ),
    x = "NO (ppm)",
    y = "REC (ug/m3)",
    color = "Measurement\nlocation"
    )
    
# Plot 6 adds facets to plot 1 + a new color theme for underground vs surface 
# Note 1:  the scales="free" option allows a different scale for each plot.  In
# this case it makes the individual plots more informative. 
# Note 2:  the smoother for facility E is behaving strangely because so many 
# of the values are equal to zero. 
# Note 3: with facets, you see facility D21 has no `nodata`
ggplot(data = DEMS, aes(nodata, ecdata, color = u_snum)) +
    geom_point() +
    facet_wrap(~ facilityno, nrow = 2, scales = "free") + 
    geom_smooth(se = FALSE) +
    labs(
    title = paste(
        "Scatterplot of the DEMS NO (ppm) vs. REC (ug/m3) data \n",
        "separately by facility and colored by where the" ,
        "measurements were taken"
    ),
    x = "NO (ppm)",
    y = "REC (ug/m3)",
    color = "Measurement\nlocation"
    ) + 
  scale_color_brewer(palette = "Dark2") +
  theme_bw()
    
```


Note:  If you make such scatterplots by facility with data on the log base 10
scale and the axes swapped, you should be able to produce plots that look like
the results in Figure 2 of [Coble et al 2010](https://academic.oup.com/annweh/article/54/7/747/202635).

---

# Homework exercises

Note:  Refer to the lab write-up guidelines *LabReportGuidelines.html* posted
on Canvas for the format and content of your lab report.

(1) Make table(s) summarizing four exposure variables in this dataset:  Nitrogen
    oxide, nitrogen dioxide, respirable organic carbon, and respirable elemental
    carbon (variables:  nodata no2data ocdata ecdata).  Include the arithmetic
    mean (AM), arithmetic standard deviation (ASD), geometric mean (GM), and
    geometric standard deviation (GSD).  Show these summaries both overall and
    at the facility level.  Write a few sentences describing the results in the
    table.  (Challenge version:  How closely can you replicate Tables 1, 2, and
    3?)

    (a) Note:  You can’t take the log of 0 so you need to decide how to handle
         the 0’s in the nitrogen oxide and organic carbon data.  Summary 
         statistics from the reduced dataset, i.e. a dataset created by omitting 
         the 0’s, will be misleading.  A simple alternative is to create a new 
         variable that adds a constant to every observation and use this 
         variable when taking logs. However, the purpose of your analysis 
         matters since adding a constant is not always the appropriate way to 
         handle this challenge.  If you do add a constant, choose the constant 
         to add thoughtfully.  How big should it be? Should it be the same for 
         NO and OC?  Make sure to document the change to the variables in your 
         lab write-up.

    (b) Getting GM and/or GSD in a table in R:  This lab doesn't show how to get
        the GM and GSD using `tidyverse`; for example using `dplyr` `group_by()` 
        and `mutate()` functions though you could (and may wish to). Instead, 
        for demonstration, we show how to use the `EnvStats` package with 
        `tapply` and `cbind`.  To get the AM, ASD, GM, GSD, N in order:
         
```{r table.with.GM+GSD}
#-----table with GM+GSD using tapply and cbind----------

#This uses facilityno, a factor variable
xbar <- tapply(DEMS$ecdata, DEMS$facilityno, mean,    na.rm=TRUE)
sd   <- tapply(DEMS$ecdata, DEMS$facilityno, sd,      na.rm=TRUE)
gm   <- tapply(DEMS$ecdata, DEMS$facilityno, geoMean, na.rm=TRUE)
gsd  <- tapply(DEMS$ecdata, DEMS$facilityno, geoSD,   na.rm=TRUE)
n    <- tapply(DEMS$ecdata[!is.na(DEMS$ecdata)],
               DEMS$facilityno[!is.na(DEMS$ecdata)], length)
cbind(AM=xbar, SD=sd, GM=gm, GSD=gsd, N=n)

# we can make this prettier using rounding and kable:
kable(round(cbind(AM=xbar, SD=sd, GM=gm, GSD=gsd, N=n), 2))

```

And repeat this exercise for underground only:  (Allows checking against Coble
Table 1, and the numbers are similar but not identical.  Without the
partitioning by location, the GSD for mine B is enormous and not that
believable.  However, I think this is the huge range of the data and not an
error.)

```{r UGtable.with.GM+GSD}
#-----UGtable.with.GM+GSD using tapply and cbind----------

# This uses facilityno, a factor variable
DEMSu <- DEMS[DEMS$u_s == "u", ]
xbar  <- tapply(DEMSu$ecdata, DEMSu$facilityno, mean,    na.rm=TRUE)
sd    <- tapply(DEMSu$ecdata, DEMSu$facilityno, sd,      na.rm=TRUE)
gm    <- tapply(DEMSu$ecdata, DEMSu$facilityno, geoMean, na.rm=TRUE)
gsd   <- tapply(DEMSu$ecdata, DEMSu$facilityno, geoSD,   na.rm=TRUE)
n     <- tapply(DEMSu$ecdata[!is.na(DEMSu$ecdata)],
                DEMSu$facilityno[!is.na(DEMSu$ecdata)], length)
cbind(AM=xbar, SD=sd, GM=gm, GSD=gsd, N=n)

# we can make this prettier using rounding and kable:
kable(round(cbind(AM=xbar, SD=sd, GM=gm, GSD=gsd, N=n), 2))

```


(2) Make some figures to show the distribution of the REC data.  (Challenge
    version: Can you do this separately by facility and location?  Can you 
    replicate Figure 1 of Coble?) Write a few sentences describing what you see 
    in the figures.

(3) Can you replicate the key information shown in Figure 2 of Coble?  What are
    your ideas for enhancing the plots? (Support your graphical presentation 
    with some brief discussion of what you see in the plots.)

---

# Appendix 1: Base R commands for reference {-}

While new packages may come and go, Base R will never go out of style. Here are 
some Base R options for tasks we've covered in this Lab.  

### A1.1 Histogram of ecdata with a density curve added to the histogram {-}

To get a basic histogram, use `hist`.  For a density plot, use `plot()` with
`density()`.  (You need to explicitly remove the missing values to use the
density function.)

```{r hist.in.baseR, echo=TRUE}
#-----hist in baseR-----------------

# plot 1: basic histogram
# can plot without explicitly excluding the missing data
hist(DEMS$ecdata) 

# plot 2: same plot, now explicitly removing missing data
hist(DEMS$ecdata[!is.na(DEMS$ecdata)]) 

# plot 3:  density plot:  need to explicitly remove missing data
plot(density(DEMS$ecdata[!is.na(DEMS$ecdata)])) 

```

### A1.2 Logarithmic transformations: {-}

(a) `DEMS$ln_ecdata <- log(DEMS$ecdata)` creates the natural log-transformed
   variable in the DEMS data frame.
   
(b) `DEMS$log10_ecdata <- log10(DEMS$ecdata)` creates the base 10 log-transformed
   variable in the DEMS data frame.

### A1.3 Basic scatterplot and overlaid regression line {-}

```{r scatterplot.in.baseR}
#-----scatterplot in baseR-------

# Plot 1 with best fit regression line

# Create a basic scatterplot
plot(DEMS$ecdata, DEMS$nodata)  

# Add a best fit regression line
abline(lm(DEMS$nodata ~ DEMS$ecdata))

# Plot 2 with scatterplot smoother
# Repeat the plot, this time with a loess smooth curve with default span
scatter.smooth(DEMS$ecdata, DEMS$nodata) 

```


# Appendix 2: Session Information, Code, and Functions {-}

The next three chunks should be included in the appendix of every R Markdown so 
that you document your session information, code, and functions defined in the 
document.  This supports the reproducibility of your work.

```{r session.info}
#-----session information: beginning of Appendix -----------

# This promotes reproducibility by documenting the version of R and every package
# you used.
sessionInfo()

```

```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, include=TRUE}
#-----appendix------------
```

```{r functions.defined.in.this.Rmd, eval = TRUE}
#-----functions defined in this Rmd ------------

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)

```
